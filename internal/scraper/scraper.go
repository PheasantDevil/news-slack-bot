package scraper

import (
	"fmt"
	"io"
	"net/http"
	"strings"
	"time"

	"github.com/PuerkitoBio/goquery"
)

type Article struct {
	Title     string
	URL       string
	Summary   string
	Thumbnail string
	PostDate  time.Time
}

// FetchNews ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†
func FetchNews(url string, targetDate time.Time) ([]Article, error) {
	// ğŸ”¥ ã‚¹ãƒ†ãƒƒãƒ—1: HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã®é€ä¿¡
	fmt.Println("Fetching URL:", url)

	// User-Agent ã‚’è¨­å®šã—ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
	client := &http.Client{}
	req, err := http.NewRequest("GET", url, nil)
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %v", err)
	}
	req.Header.Set("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64)")

	resp, err := client.Do(req)
	if err != nil {
		return nil, fmt.Errorf("failed to fetch page: %v", err)
	}
	defer resp.Body.Close()

	// ğŸ”¥ ã‚¹ãƒ†ãƒƒãƒ—2: HTTP ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèª
	fmt.Println("Response Status:", resp.Status)
	fmt.Println("Final URL after request:", resp.Request.URL.String())

	if resp.StatusCode != 200 {
		return nil, fmt.Errorf("failed to fetch page, status code: %d", resp.StatusCode)
	}

	// ğŸ”¥ ã‚¹ãƒ†ãƒƒãƒ—3: HTML ã‚’ goquery ã«æ¸¡ã™å‰ã«å‡ºåŠ›
	htmlBytes, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("failed to read response body: %v", err)
	}

	// å–å¾—ã—ãŸHTMLã‚’è¡¨ç¤º
	htmlText := string(htmlBytes)
	fmt.Println("Fetched HTML content (first 500 chars):", htmlText[:min(500, len(htmlText))])

	// goquery ã«æ¸¡ã™ãŸã‚ã«æ–°ã—ã„ Reader ã‚’ä½œæˆ
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(htmlText))
	if err != nil {
		return nil, fmt.Errorf("failed to parse HTML: %v", err)
	}
	fmt.Println("HTML parsed successfully!")

	// ğŸ”¥ ã‚¹ãƒ†ãƒƒãƒ—4: è¨˜äº‹ã‚’å–å¾—
	articles := []Article{}
	doc.Find(".p-wrap").Each(func(i int, s *goquery.Selection) {
		// ãƒ‡ãƒãƒƒã‚°: å„è¨˜äº‹ã®HTMLæ§‹é€ ã‚’å‡ºåŠ›
		html, _ := s.Html()
		fmt.Printf("Article HTML structure:\n%s\n", html)

		title := s.Find("h3.entry-title a").Text()
		url, exists := s.Find("h3.entry-title a").Attr("href")
		if !exists {
			fmt.Printf("Warning: URL not found for article %d\n", i)
		}
		summary := s.Find(".entry-summary").Text()
		thumbnail, _ := s.Find("img.featured-img").Attr("src")

		// æ—¥ä»˜ã‚’å–å¾—ã—ã¦ãƒ‘ãƒ¼ã‚¹
		dateStr := s.Find("time.date.published").AttrOr("datetime", "")
		postDate, err := time.Parse("2006-01-02T15:04:05-07:00", dateStr)
		if err != nil {
			fmt.Printf("Warning: Failed to parse date for article %d: %v\n", i, err)
			return
		}

		// æŒ‡å®šã•ã‚ŒãŸæ—¥ä»˜ã¨åŒã˜æ—¥ã®è¨˜äº‹ã®ã¿ã‚’è¿½åŠ 
		if isSameDate(postDate, targetDate) {
			fmt.Printf("Found article %d:\nTitle: %s\nURL: %s\nSummary: %s\nThumbnail: %s\nDate: %s\n\n",
				i, title, url, summary, thumbnail, postDate.Format("2006-01-02"))

			if title != "" {
				articles = append(articles, Article{
					Title:     strings.TrimSpace(title),
					URL:       url,
					Summary:   strings.TrimSpace(summary),
					Thumbnail: thumbnail,
					PostDate:  postDate,
				})
			}
		}
	})

	fmt.Printf("Total articles found: %d\n", len(articles))
	if len(articles) == 0 {
		// ãƒ‡ãƒãƒƒã‚°: å…¨ä½“ã®HTMLæ§‹é€ ã‚’ç¢ºèª
		fmt.Println("\nFull HTML structure:")
		doc.Find("body").Each(func(i int, s *goquery.Selection) {
			html, _ := s.Html()
			fmt.Printf("%s\n", html)
		})
	}

	return articles, nil
}

// isSameDate 2ã¤ã®æ™‚åˆ»ãŒåŒã˜æ—¥ä»˜ã‹ã©ã†ã‹ã‚’åˆ¤å®š
func isSameDate(t1, t2 time.Time) bool {
	y1, m1, d1 := t1.Date()
	y2, m2, d2 := t2.Date()
	return y1 == y2 && m1 == m2 && d1 == d2
}

func min(a, b int) int {
	if a < b {
		return a
	}
	return b
}
